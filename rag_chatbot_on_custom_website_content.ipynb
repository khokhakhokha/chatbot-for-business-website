{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khokhakhokha/chatbot-for-business-website/blob/main/rag_chatbot_on_custom_website_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import"
      ],
      "metadata": {
        "id": "AR2iHYbfRxAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS1DKc1Z0eTb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install  langchain-core\n",
        "!pip install langchain-google-genai\n",
        "!pip install langchain-openai\n",
        "!pip install  langchain-chroma\n",
        "!pip install chromadb\n",
        "!pip install huggingface_hub\n",
        "!pip install streamlit\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import os, tempfile, glob, random\n",
        "# documents loaders\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "# text splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "# Chroma: vectorstore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_chroma import Chroma\n",
        "# # LLM: openai and google_genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# Promt template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n"
      ],
      "metadata": {
        "id": "e7GRPLoJWHbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# from langchain.schema import BaseOutputParser\n",
        "\n",
        "# class ChatBot:\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         text_pipeline: HuggingFacePipeline,\n",
        "#         embeddings: HuggingFaceEmbeddings,\n",
        "#         training_data: list,\n",
        "#         prompt_template: str = DEFAULT_TEMPLATE,\n",
        "#         verbose: bool = False\n",
        "#     ):\n",
        "#         prompt = PromptTemplate(\n",
        "#             input_variables=[\"context\", \"question\", \"chat_history\"], template=prompt_template, output_parser=CleanupOutputParser())\n",
        "#         self.chain = self._create_chain(text_pipeline, prompt, verbose)\n",
        "#         self.db = self._embed_data(training_data, embeddings)\n",
        "\n",
        "#     def _create_chain(\n",
        "#         self,\n",
        "#         text_pipeline: HuggingFacePipeline,\n",
        "#         prompt: PromptTemplate,\n",
        "#         verbose: bool = False\n",
        "#     ):\n",
        "#         memory = ConversationBufferMemory(\n",
        "#             memory_key=\"chat_history\",\n",
        "#             human_prefix=\"### Input\",\n",
        "#             ai_prefix=\"### Response\",\n",
        "#             input_key=\"question\",\n",
        "#             output_key=\"output_text\",\n",
        "#             return_messages=False\n",
        "#         )\n",
        "#         return load_qa_chain(text_pipeline, chain_type=\"stuff\", prompt=prompt, memory=memory, verbose=verbose)\n",
        "\n",
        "#     def _embed_data(self, training_data, embeddings: HuggingFaceEmbeddings) -> Chroma:\n",
        "#         urls = training_data.urls\n",
        "#         pdfs = training_data.pdfs\n",
        "#         online_pdfs = training_data.online_pdfs\n",
        "#         documents_dir = training_data.documents_dir\n",
        "\n",
        "#         documents_loader = []\n",
        "#         if (len(documents_dir) > 0):\n",
        "#             for document_dir in documents_dir:\n",
        "#                 documents_loader.append(DirectoryLoader(\n",
        "#                     document_dir, glob=\"**/*txt\"))\n",
        "\n",
        "#         urls_loader = []\n",
        "#         if (len(urls) > 1):\n",
        "#             urls_loader.append(UnstructuredURLLoader(urls=urls))\n",
        "\n",
        "#         pdfs_loader = []\n",
        "#         if (len(pdfs) > 0):\n",
        "#             for pdf in pdfs:\n",
        "#                 pdfs_loader.append(PyPDFLoader(pdf))\n",
        "\n",
        "#         online_pdfs_loader = []\n",
        "#         if (len(online_pdfs) > 0):\n",
        "#             for online_pdf in online_pdfs:\n",
        "#                 online_pdfs_loader.append(PyPDFLoader(online_pdf))\n",
        "\n",
        "#         loaders = [urls_loader, pdfs_loader,\n",
        "#                    online_pdfs_loader, documents_loader]\n",
        "\n",
        "#         docs = []\n",
        "#         for loader in loaders:\n",
        "#             if (len(loader) > 0):\n",
        "#                 docs.extend(loader[0].load())\n",
        "#         #print('docs',docs)\n",
        "#         text_splitter = RecursiveCharacterTextSplitter(\n",
        "#             chunk_size=512, chunk_overlap=40)\n",
        "#         texts = text_splitter.split_documents(docs)\n",
        "#         return Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "#     def __call__(self, user_input: str) -> str:\n",
        "#         docs = self.db.similarity_search(user_input)\n",
        "#         return self.chain.run({\"input_documents\": docs, \"question\": user_input})\n",
        "\n",
        "\n",
        "\n",
        "# Define your base folder in Drive (you can change 'MyDrive' path if needed)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "# Create the directories if they don't exist\n",
        "(BASE_DIR / \"tmp\").mkdir(parents=True, exist_ok=True)\n",
        "(BASE_DIR / \"vector_stores\").mkdir(parents=True, exist_ok=True)\n",
        "# Define paths\n",
        "TMP_DIR = BASE_DIR / \"tmp\"\n",
        "LOCAL_VECTOR_STORE_DIR = BASE_DIR / \"vector_stores\""
      ],
      "metadata": {
        "id": "3UhUSe_8aHVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d42ab6-4355-4a0a-c316-03fed130ef94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##API KEYS"
      ],
      "metadata": {
        "id": "jXHA4-h3c2-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_api_key =  os.environ.get(\"GOOGLE_API_KEY\")\n",
        "HF_key = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")"
      ],
      "metadata": {
        "id": "w-yUy4oVdAg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Document Loaders"
      ],
      "metadata": {
        "id": "swgHaavrR6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def langchain_document_loader(TMP_DIR):\n",
        "\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    txt_loader = DirectoryLoader(\n",
        "        TMP_DIR.as_posix(), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True\n",
        "    )\n",
        "    documents.extend(txt_loader.load())\n",
        "\n",
        "    return documents"
      ],
      "metadata": {
        "id": "uPrFBaUulGsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = langchain_document_loader(TMP_DIR)\n",
        "print(f\"\\nNumber of documents: {len(documents)}\")"
      ],
      "metadata": {
        "id": "xrEHaBlFZV4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1422b25-c1e3-4bf1-f077-d9bdfdbd92d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of documents: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text splitters\n"
      ],
      "metadata": {
        "id": "qf7QqTy0Gpx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size = 1600,\n",
        "    chunk_overlap= 200\n",
        ")"
      ],
      "metadata": {
        "id": "XElVTTYXGvJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorsores and Embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mi-vrHemM2s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embeddings"
      ],
      "metadata": {
        "id": "noRqPYqmnVgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/embedding-001\",\n",
        "            google_api_key=google_api_key\n",
        "        )"
      ],
      "metadata": {
        "id": "lAkY3n5hNsop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vectorestores"
      ],
      "metadata": {
        "id": "XveMohXVnu8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vectorstore(embeddings, documents, vectorstore_name):\n",
        "    \"\"\"Create and persist a Chroma vector database.\"\"\"\n",
        "\n",
        "    # Define persistence directory\n",
        "    persist_directory = Path(\"./data/vector_stores\") / vectorstore_name\n",
        "    persist_directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create Chroma vector store\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=str(persist_directory)\n",
        "    )\n",
        "\n",
        "    # Persist data to disk\n",
        "    vector_store.persist()\n",
        "\n",
        "\n",
        "    return vector_store, str(persist_directory)"
      ],
      "metadata": {
        "id": "Uv_iM1n4n3YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load chroma vectorestore"
      ],
      "metadata": {
        "id": "DP_rngeq_iYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma(\n",
        "    persist_directory = LOCAL_VECTOR_STORE_DIR.as_posix() + \"/Vit_All_Google_Embeddings\",\n",
        "    embedding_function=embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "c5SfybDc_rV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrivers"
      ],
      "metadata": {
        "id": "TAWtkNAsXcD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorestore_retriver():\n",
        "  return vector_store.as_retriever()\n"
      ],
      "metadata": {
        "id": "IDTANyF0XhWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrival : all the block"
      ],
      "metadata": {
        "id": "N3FsniHtgssG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_retrieval_pipeline(\n",
        "    create_new_vectorstore=True,\n",
        "    vectorstore_name=\"Vit_All_Google_Embeddings\",\n",
        "    chunk_size=1600,\n",
        "    chunk_overlap=200,\n",
        "    google_api_key=\"***\",\n",
        "):\n",
        "    \"\"\"\n",
        "    ðŸ” Sets up the retrieval pipeline:\n",
        "    Loads documents â†’ splits into chunks â†’ embeds them â†’ creates or loads a Chroma vectorstore â†’ builds a retriever.\n",
        "\n",
        "    Parameters:\n",
        "        create_new_vectorstore (bool):\n",
        "            If True, creates a new Chroma vectorstore from source documents.\n",
        "            If False, loads an existing vectorstore from disk.\n",
        "\n",
        "        vectorstore_name (str):\n",
        "            Directory name (inside LOCAL_VECTOR_STORE_DIR) where the Chroma vectorstore is stored.\n",
        "\n",
        "        chunk_size (int):\n",
        "            Maximum number of characters per text chunk when splitting documents.\n",
        "\n",
        "        chunk_overlap (int):\n",
        "            Number of overlapping characters between consecutive chunks to preserve context.\n",
        "\n",
        "        google_api_key (str):\n",
        "            Google API key used to initialize Google Generative AI embeddings.\n",
        "\n",
        "    Returns:\n",
        "        retriever (BaseRetriever):\n",
        "            A configured retriever ready to query the Chroma vectorstore.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # --- Initialize Embeddings ---\n",
        "        print(\"ðŸ§  Initializing Google Generative AI Embeddings ...\")\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/embedding-001\",\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "\n",
        "        # --- Create or Load Vectorstore ---\n",
        "        if create_new_vectorstore:\n",
        "            print(f\"ðŸ”§ Creating new Chroma vectorstore: {vectorstore_name} ...\")\n",
        "\n",
        "            # 1ï¸âƒ£ Load documents\n",
        "            documents = langchain_document_loader(TMP_DIR)\n",
        "\n",
        "            # 2ï¸âƒ£ Split into chunks\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap\n",
        "            )\n",
        "            chunks = splitter.split_documents(documents)\n",
        "\n",
        "            # 3ï¸âƒ£ Create and persist vectorstore\n",
        "            vector_store = create_vectorstore(\n",
        "                embeddings=embeddings,\n",
        "                documents=chunks,\n",
        "                vectorstore_name=vectorstore_name,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(f\"ðŸ“¦ Loading existing Chroma vectorstore: {vectorstore_name} ...\")\n",
        "            vector_store = Chroma(\n",
        "                persist_directory=(LOCAL_VECTOR_STORE_DIR / vectorstore_name).as_posix(),\n",
        "                embedding_function=embeddings\n",
        "            )\n",
        "\n",
        "        # --- Build Retriever ---\n",
        "        retriever = vectorestore_retriver()\n",
        "\n",
        "        # --- Summary Output ---\n",
        "        chunk_count = vector_store._collection.count()\n",
        "        print(f\"\\nâœ… Retrieval pipeline ready!\")\n",
        "        print(f\"   âž¤ Vectorstore: {vectorstore_name}\")\n",
        "        print(f\"   âž¤ Embeddings: Google Generative AI\")\n",
        "        print(f\"   âž¤ Chunks available: {chunk_count}\")\n",
        "\n",
        "        return retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error in setup_retrieval_pipeline: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "-cfzwCL3gxU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatmodel"
      ],
      "metadata": {
        "id": "QDaDItYZOUUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def instantiate_llm(api_key,  temperature=0.5, top_p=0.95):\n",
        "    \"\"\"\n",
        "    ðŸ§  Instantiate a Google Generative AI (Gemini) LLM in LangChain.\n",
        "\n",
        "    Parameters:\n",
        "        api_key (str):\n",
        "            Your Google API key used to authenticate with the Gemini model.\n",
        "        temperature (float):\n",
        "            Controls randomness in responses. Range: 0.0â€“1.0 (default=0.5).\n",
        "        top_p (float):\n",
        "            Nucleus sampling parameter. Range: 0.0â€“1.0 (default=0.95).\n",
        "\n",
        "    Returns:\n",
        "        ChatGoogleGenerativeAI:\n",
        "            A configured Google Generative AI LLM ready for use with LangChain.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        google_api_key=api_key,\n",
        "        model='gemini-pro',\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        convert_system_message_to_human=True,\n",
        "    )\n",
        "\n",
        "    return llm\n"
      ],
      "metadata": {
        "id": "s0JrRSo8OW4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Memory"
      ],
      "metadata": {
        "id": "_RqY-muAgWQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_memory():\n",
        "    \"\"\"\n",
        "    Creates a ConversationBufferMemory that stores the full chat history\n",
        "    (no summarization, all messages kept as-is).\n",
        "    Suitable for models with large context windows.\n",
        "    \"\"\"\n",
        "    memory = ConversationBufferMemory(\n",
        "        return_messages=True,\n",
        "        memory_key='chat_history',\n",
        "        output_key=\"answer\",\n",
        "        input_key=\"question\",\n",
        "    )\n",
        "    return memory\n"
      ],
      "metadata": {
        "id": "IjT0OaCigYVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt template"
      ],
      "metadata": {
        "id": "0WyiMc4kijyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_TEMPLATE = \"\"\"\n",
        "Your name is **Sofia**. You are a friendly and knowledgeable customer support agent at **PureSkin**, a natural skincare brand founded in **2020**. You are chatting with a customer.\n",
        "\n",
        "**Context:** {context}\n",
        "\n",
        "If you donâ€™t know the answer, politely suggest contacting **support@pureskin.com** for more information.\n",
        "Keep your replies **short, compassionate, and informative**.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Response:**\n",
        "\"\"\".strip()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKNIdG_KlXBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chat prompt\n",
        "prompt = ChatPromptTemplate.from_template(DEFAULT_TEMPLATE)"
      ],
      "metadata": {
        "id": "uQ6386B74Hf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational retrieval Chain"
      ],
      "metadata": {
        "id": "WbF1FiIO69aF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmwRoqudN541"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}