{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khokhakhokha/chatbot-for-business-website/blob/main/Fixed_LangChain_Notebook_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f296ed3c",
      "metadata": {
        "id": "f296ed3c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================\n",
        "# ðŸ“¦ INSTALL ALL REQUIRED PACKAGES (STABLE SETUP)\n",
        "# ============================================\n",
        "# !pip uninstall -y langchain langchain-core langchain-community langchain-text-splitters\n",
        "# !pip install -q langchain==0.0.350 langchain-google-genai chromadb unstructured tiktoken nbformat streamlit\n",
        "!pip install langchain==0.1.4\n",
        "!pip install langchain-google-genai==0.0.6\n",
        "!pip install chromadb\n",
        "!pip install unstructured\n",
        "!pip install tiktoken\n",
        "!pip install nbformat\n",
        "!pip install streamlit==1.28.0\n",
        "print(\"âœ… Installation complete. Please restart runtime before continuing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports_section",
      "metadata": {
        "id": "imports_section"
      },
      "source": [
        "## 2. Import All Dependencies\n",
        "\n",
        "Import all necessary libraries and define helper functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CORE PYTHON LIBRARIES\n",
        "# ============================================\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "# ============================================\n",
        "# LANGCHAIN CORE COMPONENTS\n",
        "# ============================================\n",
        "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import HumanMessage, AIMessage, BaseMessage\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# ============================================\n",
        "# LANGCHAIN CHAIN COMPONENTS\n",
        "# ============================================\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# ============================================\n",
        "# GOOGLE GEMINI INTEGRATION\n",
        "# ============================================\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================\n",
        "def require_env_var(name: str):\n",
        "    val = os.environ.get(name)\n",
        "    if not val:\n",
        "        raise EnvironmentError(f\"Environment variable {name} is required. Set it and re-run the cell.\")\n",
        "    return val\n",
        "\n",
        "print(\"âœ… All dependencies imported successfully and ready to use!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkvZ99En_QD6",
        "outputId": "bfd29731-8216-4e7e-9c5c-ddc84d044d69"
      },
      "id": "rkvZ99En_QD6",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All dependencies imported successfully and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyC4frdIQu2rZ2BDutQZuOayDii8ioXCoSw'"
      ],
      "metadata": {
        "id": "GGYK9nUi67bL"
      },
      "id": "GGYK9nUi67bL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "document_loading_section",
      "metadata": {
        "id": "document_loading_section"
      },
      "source": [
        "## 3. Document Loading and Splitting\n",
        "\n",
        "Functions to load documents from various formats and split them into manageable chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fe8be39",
      "metadata": {
        "id": "0fe8be39"
      },
      "outputs": [],
      "source": [
        "def load_documents(directory: str = './docs') -> List[Any]:\n",
        "    p = Path(directory)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Documents directory not found: {p.resolve()}\")\n",
        "\n",
        "    docs = []\n",
        "    for f in p.rglob('*'):\n",
        "        if f.is_dir():\n",
        "            continue\n",
        "\n",
        "        ext = f.suffix.lower()\n",
        "        try:\n",
        "            if ext in ('.txt', '.md', '.csv', '.json'):\n",
        "                loader = TextLoader(str(f), encoding='utf-8')\n",
        "                docs.extend(loader.load())\n",
        "            elif ext == '.pdf':\n",
        "                loader = UnstructuredPDFLoader(str(f))\n",
        "                docs.extend(loader.load())\n",
        "            else:\n",
        "                try:\n",
        "                    loader = TextLoader(str(f), encoding='utf-8')\n",
        "                    docs.extend(loader.load())\n",
        "                except Exception:\n",
        "                    print(f'Skipping unsupported file: {f.name}')\n",
        "        except Exception as e:\n",
        "            print(f'Failed to load {f.name}: {e}')\n",
        "\n",
        "    print(f'Loaded {len(docs)} documents from {directory}')\n",
        "    return docs\n",
        "\n",
        "def split_documents(documents: List[Any], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Any]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(documents)\n",
        "    print(f'Split into {len(chunks)} chunks')\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vectorstore_section",
      "metadata": {
        "id": "vectorstore_section"
      },
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def get_embeddings():\n",
        "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31833b42",
      "metadata": {
        "id": "31833b42"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_chroma(documents, persist_directory=\"./chroma_db\", embedding=None):\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents,\n",
        "        embedding,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    vectordb.persist()\n",
        "    return vectordb\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Any\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Function to get Hugging Face embeddings\n",
        "def get_hf_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> HuggingFaceEmbeddings:\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Build Chroma vectorstore using HF embeddings\n",
        "def build_chroma(texts: List[Any], persist_directory: str = './chroma_db', embedding: Optional[HuggingFaceEmbeddings] = None) -> Chroma:\n",
        "    embedding = embedding or get_hf_embeddings()\n",
        "    vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
        "    print(f'Chroma vectorstore ready at {persist_directory}')\n",
        "    return vectordb\n",
        "\n",
        "# Load persisted Chroma vectorstore using HF embeddings\n",
        "def load_chroma(persist_directory: str = './chroma_db', embedding: Optional[HuggingFaceEmbeddings] = None) -> Chroma:\n",
        "    embedding = embedding or get_hf_embeddings()\n",
        "    p = Path(persist_directory)\n",
        "    if not p.exists() or not any(p.iterdir()):\n",
        "        raise FileNotFoundError(f'Chroma persistence directory not found at {persist_directory}. Run document ingestion first.')\n",
        "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "    print(f'Loaded persisted Chroma from {persist_directory}')\n",
        "    return vectordb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llm_section",
      "metadata": {
        "id": "llm_section"
      },
      "source": [
        "## 5. Google Gemini Chat Model Configuration\n",
        "\n",
        "Initialize the Google Gemini language model for conversational AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4634bed9",
      "metadata": {
        "id": "4634bed9"
      },
      "outputs": [],
      "source": [
        "def get_google_chat_model(temperature: float = 0.2, model_name: Optional[str] = None) -> ChatGoogleGenerativeAI:\n",
        "    require_env_var('GOOGLE_API_KEY')\n",
        "    model_name = model_name or os.environ.get('GOOGLE_MODEL', 'gemini-2.5-flash')\n",
        "    return ChatGoogleGenerativeAI(temperature=temperature, model=model_name,convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag_chain_section",
      "metadata": {
        "id": "rag_chain_section"
      },
      "source": [
        "## 6. RAG Chain Construction\n",
        "\n",
        "Build the complete RAG chain with history-aware retrieval and document combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31ca582b",
      "metadata": {
        "id": "31ca582b"
      },
      "outputs": [],
      "source": [
        "def build_rag_chain(vectordb: Chroma, llm: Optional[ChatGoogleGenerativeAI] = None, k: int = 4):\n",
        "    llm = llm or get_google_chat_model()\n",
        "    retriever = vectordb.as_retriever(search_kwargs={'k': k})\n",
        "    rag = ConversationalRetrievalChain.from_llm(llm, retriever, return_source_documents=True)\n",
        "    print('RAG chain built successfully using ConversationalRetrievalChain')\n",
        "    return {'chain': rag}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ingestion_section",
      "metadata": {
        "id": "ingestion_section"
      },
      "source": [
        "## 7. Document Ingestion and Indexing\n",
        "\n",
        "Load documents, split them into chunks, and create the vector store index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "60265fc6",
      "metadata": {
        "id": "60265fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200c3ec3-4d42-4942-bfec-6a657b0db1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents...\n",
            "Loaded 6 documents from ./docs\n",
            "\n",
            "Splitting documents...\n",
            "Split into 13 chunks\n",
            "\n",
            "Creating embeddings and building vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma vectorstore ready at ./chroma_db\n",
            "\n",
            "Document ingestion complete!\n"
          ]
        }
      ],
      "source": [
        "docs_dir = './docs'\n",
        "persist_dir = './chroma_db'\n",
        "\n",
        "print('Loading documents...')\n",
        "docs = load_documents(docs_dir)\n",
        "\n",
        "print('\\nSplitting documents...')\n",
        "chunks = split_documents(docs, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "print('\\nCreating embeddings and building vector store...')\n",
        "emb = get_hf_embeddings()   # <-- use HF embeddings here\n",
        "vectordb = build_chroma(chunks, persist_directory=persist_dir, embedding=emb)\n",
        "\n",
        "print('\\nDocument ingestion complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat history"
      ],
      "metadata": {
        "id": "xzMUqZJ7Djq6"
      },
      "id": "xzMUqZJ7Djq6"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "class ChatHistory:\n",
        "    def __init__(self):\n",
        "        self.messages = []\n",
        "\n",
        "    def add_user_message(self, text):\n",
        "        self.messages.append(HumanMessage(content=text))\n",
        "\n",
        "    def add_ai_message(self, text):\n",
        "        self.messages.append(AIMessage(content=text))\n",
        "\n",
        "    def get_messages(self):\n",
        "        # returns list of BaseMessage objects\n",
        "        return self.messages\n",
        "\n"
      ],
      "metadata": {
        "id": "QJH6z1d7Dm1-"
      },
      "id": "QJH6z1d7Dm1-",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "chat_section",
      "metadata": {
        "id": "chat_section"
      },
      "source": [
        "## 8. Interactive Chat Interface\n",
        "\n",
        "Test the RAG chatbot with an interactive terminal-based chat loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c1210067",
      "metadata": {
        "id": "c1210067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "9cbe83de-cf0c-44ba-99fd-aa1d0667162a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing chatbot...\n",
            "Loaded persisted Chroma from ./chroma_db\n",
            "RAG chain built successfully using ConversationalRetrievalChain\n",
            "\n",
            "Chatbot ready! Type exit or quit to stop.\n",
            "\n",
            "============================================================\n",
            "\n",
            "You: what\n",
            "\n",
            "Assistant: I don't know the answer because your question is incomplete. Please tell me what information you are looking for.\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "You: the company u are working for\n",
            "\n",
            "Assistant: I am working for PureSkin.\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2100398486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nYou: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "print('Initializing chatbot...')\n",
        "\n",
        "try:\n",
        "    vectordb = load_chroma(persist_dir)\n",
        "except:\n",
        "    print('Could not load vector store. Run the ingestion cell first.')\n",
        "    raise\n",
        "\n",
        "llm = get_google_chat_model()\n",
        "rag_bundle = build_rag_chain(vectordb, llm=llm, k=4)\n",
        "rag_chain = rag_bundle['chain']\n",
        "\n",
        "chat_history = ChatHistory()\n",
        "\n",
        "print('\\nChatbot ready! Type exit or quit to stop.\\n')\n",
        "print('=' * 60)\n",
        "\n",
        "while True:\n",
        "    user_input = input('\\nYou: ').strip()\n",
        "\n",
        "    if user_input.lower() in ('exit', 'quit', 'q'):\n",
        "        print('\\nGoodbye!')\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    result = rag_chain.invoke({'question': user_input, 'chat_history': chat_history.get_messages()})\n",
        "    answer = result.get('answer', 'No answer generated.')\n",
        "\n",
        "    print(f'\\nAssistant: {answer}')\n",
        "\n",
        "    chat_history.add_user_message(user_input)\n",
        "    chat_history.add_ai_message(answer)\n",
        "\n",
        "    context_docs = result.get('context', [])\n",
        "    if context_docs:\n",
        "        print('\\nSources:')\n",
        "        for i, doc in enumerate(context_docs[:3], 1):\n",
        "            source = doc.metadata.get('source', 'unknown')\n",
        "            preview = doc.page_content[:150].replace('\\n', ' ')\n",
        "            print(f'  {i}. {source}: {preview}...')\n",
        "\n",
        "    print('\\n' + '-' * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}